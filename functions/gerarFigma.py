import json
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Caminho para o modelo fine-tunado
model_path = "C:\\Users\\Autaza\\Documents\\Fatec\\Bertoti\\fine_tuned_gpt2"

# Carregar modelo e tokenizer
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token  # Necess√°rio para compatibilidade

# Fun√ß√£o para gerar sa√≠da
def gerar_json_wireframe(prompt):
    prompt_formatado = f"{prompt}\n"  # Formata√ß√£o do prompt
    inputs = tokenizer.encode(prompt_formatado, return_tensors="pt")
    
    # Cria√ß√£o da aten√ß√£o de m√°scara
    attention_mask = (inputs != tokenizer.pad_token_id).long()

    # Gera√ß√£o com m√°scara de aten√ß√£o
    outputs = model.generate(
        inputs,
        max_length=512,
        num_return_sequences=1,
        do_sample=True,            # Sampling ajuda a evitar repeti√ß√µes
        top_k=20,                  # Limita a escolha de tokens aos top 50
        top_p=0.95,                # Nucleus sampling
        temperature=0.2,           # Aleatoriedade controlada
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
        attention_mask=attention_mask  # Passa a m√°scara de aten√ß√£o
    )

    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Extrair somente o que vem ap√≥s o prompt
    resposta_limpa = resposta.replace(prompt_formatado, "").strip()

    print("üß† Sa√≠da bruta do modelo:\n", resposta_limpa)

    # Tenta converter a parte √∫til em JSON
    try:
        return json.loads(resposta_limpa)
    except json.JSONDecodeError:
        return {"error": "N√£o foi poss√≠vel gerar um JSON v√°lido", "raw": resposta_limpa}

# Teste
prompt = "Crie um wireframe de uma p√°gina de cadastro com nome, email e senha."
resultado = gerar_json_wireframe(prompt)

print("\n‚úÖ Resultado final:")
print(json.dumps(resultado, indent=2))
